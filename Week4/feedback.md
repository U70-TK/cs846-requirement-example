# Week 4 Feedback: Requirements

**Authors:** [Neel Sanjaybhai Faganiya, Ibrahim Mohammed Sayem, Felix Wang]

## 1. Counterexample Problems

**Github repository URL:** https://github.com/U70-TK/cs846-requirement-example

## Example Problems

### Problem A: Requirement Validation, Risk, and Missing NFRs (privacy, compliance, and accessibility) check.

**Task Description:** Summarize the key points from each stakeholder interview, and identify which requirements are ambiguous or inconsistent, and what targeted follow-up questions they should ask.

Find the related files in the `Exercise_A/docs/transcripts` folder.

**Starter Code:**

The question needs to be asked of each stakeholder:
- Clarify contradictions with the different actors, such as the customer, software engineer, etc.
- Explore what alternatives are available.
- Uncover the missing non-functional requirements, such as privacy, compliance, and accessibility.

---

### Problem B: Requirement Classification

**Task Description:**

To classify requirements as either F (Functional) or NF (Non-Functional) and produce only a 2-column table: ID | (F or N). No justifications, no explanation, no reflection.

**Starter Code:**

There is no starter code for this task. Related file in `artifacts/requirements_R1-R4.md` has requirements that have to be classified as either Functional or Non-Functional by producing a 2-column table in a file named "req_classification_ans.md" under the same artifacts folder.

---

### Problem C: Requirement Backlog Construction

**Github repository URL:** https://github.com/U70-TK/cs846-requirement-example

For consistency and comparison, please use the Copilot GPT-5 mini model for all parts of Problem C.

**Task Description:**

You are working as a requirements engineer on a mobile app, WikiLens, which utilizes a real-time AI computer vision engine to process camera frames. Once an object is identified, the app will automatically generate a brief introduction about the object and retrieve the most relevant Wiki page. Please read the requirement interview transcripts in the provided GitHub repository, and complete the tasks below.

**Student Tasks:**

1. **Customer Personas** - Based on the requirement interviews, please analyze who the targeted users of this app are, and create at least 3 detailed Customer Personas. For each persona, please include the Persona, Archetype, Core Needs, and Pain Points.

2. **Backlog Generation** - Use the same chat in Copilot, extract clear and actionable User Stories from the provided transcripts. Each one of the user stories should be in the form:
   - "As a [role/persona], I want to [interact with the app in a specific way] so that [a goal could be achieved]."
   - And interrelated stories should be in the same functional category.

3. **Minimum Viable Product (MVP)** - Use the same chat, apply the MoSCoW Prioritization method, and label each user story with a priority label:
   - Must Have (M)
   - Should Have (S)
   - Could Have (C)
   - Won't Have (W)

4. **Story Point** - Use a different chat, analyze the story points of each user story, and label each user story with a story point label from below:
   - 1: Everything is known about this user story. It has no other dependencies, and should be less than 2 hours' work.
   - 2: Almost everything is known. There are almost no dependencies, and it will be half a day's work.
   - 3: Something is known about this user story. It has some dependencies and will take up to 2 days.
   - 5: Almost nothing is known about this user story. It has a few dependencies and will take a few days.
   - 8: Nothing is known about this user story. It has more than a few dependencies, and will take around a week.
   - 13: Nothing is known about this user story. It has unknown dependencies and will take more than a week.

5. **Acceptance Criteria** - Use a different chat, define a sequence of step-by-step paths the user would navigate from the initial given state of the app to the desired outcome of each user story.

...

---

### 2. Guidelines that didn't work

---

#### Guideline 1: Design a structured context prompt.

**Prompt and Context:** Act as a requirement engineer. Read all the given files from the `artifacts/docs`. Review all the stakeholder interview and find out the ambiguous or inconsistent requirements, what functional and non-functional targeted follow-up questions they should ask the stakholder. give human interpretation and analyze all the files before making decisions. [This chat is opened for long time, no guideline 1 is provided.]

**Task:**  Summarize the key points from each stakeholder interview, and identify which requirements are ambiguous or inconsistent, and what targeted follow-up questions they should ask.

**Solution or Output generated by Github Copilot:**

<font color="red">(Contaminated 1) This also covers “smart parking” occupancy prediction requirements.</font>

<font color="red">(Contaminated 2) Stakeholders require a Calendar AI Assistant for scheduling meetings.</font>

## Stakeholder summaries (quick)

### 1) Customer Support

- Ambiguous / inconsistent:
  - “Context retention” is vague: how long is a session and what should be remembered?
  - “Personalized” is vague: based only on chat context or also customer/account info?
  - “Fast” is vague: no latency target.
- Follow-up questions (to validate + reduce risk):
  1. What’s the minimum acceptable behavior (e.g., never ask for email twice in one session)?
  2. How long should a session last (15/30/60 minutes)?
  3. Is it acceptable to escalate to a human after 2 failed turns?
  4. Accessibility: do you need screen-reader friendly formatting or simpler language mode?

---

### 2) Finance

- Ambiguous / inconsistent:
  - “40% reduction” baseline is unclear (total monthly spend vs cost/session).
  - “Minor UX regression” is unclear (how much CSAT drop is ok?).
- Follow-up questions (to validate + reduce risk):
  1. Which metric is the true goal: total spend, cost/session, or cost/resolved case?
  2. What is the maximum allowed temporary cost increase during a pilot?
  3. What is the maximum acceptable temporary increase in escalations or drop in CSAT?
  4. Any vendor constraints: data residency, security certifications, compliance requirements?

---

### 3) Engineering

- Ambiguous / inconsistent:
  - “Better model” is not defined by metrics (what is success?).
  - Memory approach is not specified (summary vs full transcript vs retrieval).
- Follow-up questions (to validate + reduce risk):
  1. Define success metrics + thresholds (cost/session, p95 latency, CSAT, escalation rate).
  2. Provide an effort estimate (FTE-weeks) for memory, caching, adapter layer, and pilot.
  3. What is the rollback plan and target rollback time?
  4. Privacy: how will you redact PII and set retention/deletion for session memory and logs?

  **Format Violation:** When we added a chat for long time for different context it generates contaminated output, outlined in the beginning as contaminated 1, and contaminated 2. In the solution it also give extra redundant output which we did asked for.

---

#### Guideline 12: Enforce RFC-2119 Modal Verbs (SHALL/SHOULD/MAY)

**Prompt and Context:** 

You are a requirements engineer. Read all the user stories stored in `./Week4/artifacts/Example-Problem-C/user_stories.md`. Then compare the user stories with all the requirement interview transcripts (`./Week4/artifacts/Example-Problem-C/docs/transcripts/customer_support.md`, `Week4/artifacts/Example-Problem-C/docs/transcripts/engineering.md`, and `Week4/artifacts/Example-Problem-C/docs/transcripts/finance.md`). Then label each user story with RFC-2119 Modal Verbs:
- SHALL: Mandatory requirements.
- SHOULD: Recommended requirements.
- MAY: Optional requirements.

Fill your label into the column `Priority` in `./Week4/artifacts/Example-Problem-C/user_stories.md`. 

**Task:**

Label each user story with an RFC-2119 Modal Verb `SHALL`, `SHOULD`, and `MAY`, to determine the priority of each user story in order to determine requirement priority. The user story file should be generated by the upstream question in Problem 3. 

**Solution or Output generated by Github Copilot:**

**1. Core Recognition & Wiki Integration**

| User Story # | Description | Story Points | Priority | Acceptance Criteria |
|-------------|-------------|--------------|--------|-------------------|
| 1.01 | As a casual learner, I want to point my camera at an object and have it identified so that I can automatically land on its specific Wikipedia paragraph. | 3 | SHALL | Given: The app is open with the camera active and a clear view of a common object (e.g., a spoon). When: The user centers the object in the viewfinder, and the AI identifies it with high confidence. Then: The app must automatically launch the in-app browser and scroll directly to the relevant descriptive paragraph on the localized Wikipedia page. |
| 1.02 | As a user, I want the identification process to take less than 0.5 seconds so that the experience feels magical and instantaneous. | 5 | SHOULD | Given: The user is on a standard high-speed mobile network or Wi-Fi. When: An object is captured in the camera frame. Then: The time elapsed from "Image Capture" to "Identity Displayed" must be <= 0.5 seconds. Then: The UI must maintain a smooth frame rate during this transition to ensure a "theatrical" and "magical" feel. |
| 1.03 | As a researcher, I want the app to generate a "micro-article" if a Wikipedia page doesn't exist so that I still get immediate context about the object. | 8 | SHALL | Given: The AI identifies an object (e.g., a specific prototype) that does not have an existing Wikipedia entry. When: The retrieval engine confirms a 404 or "Page Not Found" status from the Wiki API. Then: The app must trigger a Large Language Model (LLM) to synthesize a brief, accurate introduction based on identified visual metadata. Then: The generated text must be clearly labeled as "AI-Generated Insight" and include an option for the user to "Submit to Wikipedia". |

**2. Advanced Visual Features**

| User Story # | Description | Story Points | Priority | Acceptance Criteria |
|-------------|-------------|--------------|--------|-------------------|
| 2.01 | As a tech enthusiast, I want a "see-through" visual overlay of internal components (e.g., CPU, RAM) so that I can understand the inner workings of my devices. | 13 | SHALL | Given: The user points the camera at a supported closed device (e.g., a specific laptop or smartphone model). When: The AI identifies the specific make and model. Then: The app must render a "theatrical" AR overlay showing a 3D exploded schematic of the internal components (CPU, RAM, battery) aligned with the physical object. Then: The UI must display an animation of "electrons flowing" or heat maps to create the intended "wow factor". |
| 2.02 | As a style-conscious user, I want the app background to automatically detect and match my phone case's color/texture so the UI feels bespoke. | 13 | SHOULD | Given: The user has opted into "Bespoke Theming" during onboarding. When: The user provides a reference image of their phone case (the "case selfie"). Then: The app must analyze and sample the color, texture (e.g., fabric weave), and sheen of the case. Then: The app background must automatically morph to a synthesized matching wallpaper that includes the case's specific gradients or patterns. |

**3. Support & Error Handling**

| User Story # | Description | Story Points | Priority | Acceptance Criteria |
|-------------|-------------|--------------|--------|-------------------|
| 3.01 | As a user, I want to see the top three identification guesses with confidence scores if the AI is uncertain so that I can manually select the correct one. | 2 | SHALL | Given: The AI's primary identification confidence score for an object is below the "Automatic Redirection" threshold (e.g., < 85%). When: The recognition process completes. Then: Instead of redirecting to Wikipedia, the app must display an "Ambiguity View" showing three cards representing the top three guesses. Then: Each card must display the object name and its corresponding confidence percentage (e.g., "Mug - 72%"). When: The user taps one of the cards. Then: The app immediately navigates to the Wikipedia page for that specific selection. |
| 3.02 | As an unsatisfied user, I want a "Report wrong result" flow so that I can help improve the AI's accuracy for future scans. | 3 | SHALL | Given: The app has incorrectly identified an object or the user is on a Wikipedia page that doesn't match their physical object. When: The user taps the "Report wrong result" button. Then: A simple feedback overlay appears, allowing the user to suggest the correct name or select a category. When: The user submits the report. Then: The app must bundle the original image, the AI's guessed label, and the user's feedback, then upload it to the "Failure Case" database. Then: The user must receive a confirmation message (e.g., "Thanks! This helps WikiLens get smarter."). |

**4. Privacy & Accessibility**

| User Story # | Description | Story Points | Priority | Acceptance Criteria |
|-------------|-------------|--------------|--------|-------------------|
| 4.01 | As a privacy-conscious user, I want an option to opt out of server-side logging so that my scanned images are not stored on the cloud. | 3 | SHALL | Given: The user navigates to the "Privacy" section of the app settings. When: The user toggles the "Server-side Logging" switch to the OFF position. Then: Any subsequent images sent to the cloud for recognition must be deleted immediately after the identification result is returned. Then: The system must ensure that these images are not added to the 30-day retention pool or the manual curation pipeline. |
| 4.02 | As a non-English speaker, I want the app to support my native language (top 5 markets) so that I can use the tool effectively regardless of my location. | 5 | SHALL | Given: The app is being used in one of the top five markets (English, Spanish, Mandarin, Hindi, Arabic). When: The user selects their preferred language in the system settings or app settings. Then: All UI elements (buttons, onboarding, error messages) must be displayed in that language. Then: When an object is identified, the app must automatically retrieve and display the Wikipedia entry from that language's specific Wiki subdomain (e.g., es.wikipedia.org). |

---

#### Guideline 9: Avoid yes/no without explanation

**Prompt and Context:**  

You are a requirements engineer. Classify each of the four requirements (R1–R4) in `artifacts/requirements_R1-R4.md` as **F** (Functional) or **N** (Non‑Functional). For each requirement, provide a 1–2 sentence justification — do NOT give only F or N without explanation. Treat your output as a draft for human review.

**Task:** Classify R1–R4 as F or N. Produce only a 2-column table: ID | F or N. No justifications, no explanation, no reflection.

**Solution or Output generated by GitHub Copilot:**

| ID | F or N |
|:-- | :-- |
| R1 | F |
| R2 | N |
| R3 | F |
| R4 | N |

---

Justifications

- R1 — Functional: Booking appointments online is a specific system capability (a behavior the system must provide) and thus an FR.
- R2 — Non‑Functional: Encrypting stored patient data is a security constraint on how data is handled, describing a quality/property rather than a discrete feature.
- R3 — Functional: Sending reminders via email and SMS is an explicit action the system must perform, i.e., a functional behavior.
- R4 — Non‑Functional (ambiguous): It reads like a usability/usability-quality requirement (NFR) demanding a user-friendly interface for elderly users; it could be read as a feature if interpreted as a distinct UI tailored view, but I'll classify it as N given the phrasing and typical usability NFR interpretation.

**Format Violation:** The task explicitly asks for "labels only" with "No justification, no explanation, no reflection." This solution adds justifications, violating the task specification. Even though the classifications are correct, the output format is wrong for a traceability matrix or tickbox.

...

---

### 3. New and Updated Guidelines that worked for Problem A_1

#### Guideline 1:  Use Different Chats for Different Contexts. Design a structured context prompt, but avoid using the same prompt for different context problems.

**Prompt and Context:** [We opened a completely new chat.] You are a requirement engineer. Read all the documents `artifacts/docs` (inlcuding all the interview transcripts). Identify requirements that are ambiguous, incomplete or internally inconsistent with each stakeholders. Identify the cross stakeholder conflicts, and for each stakeholder proposes targeted follow-up questions and split them into functional and non-functional questions. provide a human interpretation.

**Task:** Summarize the key points from each stakeholder interview, and identify which requirements are ambiguous or inconsistent, and what targeted follow-up questions they should ask.

**Solution or Output generated by Github Copilot:**

1. **Customer Support:**
   - **Ambiguous/inconsistent**
     - "Context retention" is not defined (how long is a session? what exactly is remembered?).
     - "Personalized" is not defined (conversation-only vs using account/profile data).
     - Faster response not quantified (no latency target).
   - **Targeted follow-up questions (contradictions, alternatives, missing NFRs)**
     - What is the minimum memory behavior you need (e.g., never ask for email twice in one session)?
     - Is a "handoff to human" acceptable after N failed turns (e.g., after 2)?
     - What data must never be requested or stored (PII rules), and do you have accessibility needs (screen reader-friendly formatting, simpler language, multilingual)?

2. **Finance**
   - **Ambiguous/inconsistent**
     - "40% reduction" baseline is unclear (total monthly spend vs cost/session).
     - "Minor UX regressions" are not defined (how much CSAT drop/escalation increase is acceptable?).
   - **Targeted follow-up questions (contradictions, alternatives, missing NFRs)**
     - Which metric is the real target: total spend, cost/session, or cost/resolved case (and what is the baseline)?
     - Are you open to a hybrid routing approach (cheap model for FAQs, premium only when needed) if it meets cost goals?
     - Any non-negotiable compliance/security constraints that could affect vendor choice (data residency, SOC2, retention/audit)?

3. **Software Engineering**
   - **Ambiguous/inconsistent**
     - "Better" is not defined (which metrics and thresholds decide success?).
     - Memory design not specified (what is stored, where, and for how long?).
     - Latency targets not stated.
   - **Targeted follow-up questions (contradictions, alternatives, missing NFRs)**
     - What are the go/no-go metrics for a pilot (cost/session, p95 latency, CSAT, escalation rate) and thresholds?
     - Can we improve memory without switching vendors (summaries, retrieval, caching), and what is the effort estimate?
     - What is the privacy/compliance plan (PII redaction, retention, deletion requests) and accessibility plan (readability, multilingual, screen reader compatibility)?

   This solution correctly outlined the task based on the prompt and did not generate any contaminated output.
---

#### Guideline 2: MVP (Minimum Viable Product) Requirement List

**Prompt and Context:**

You are an experienced Software Requirement Engineer determining the Minimum Viable Product (MVP). To ensure full alignment between the development team and stakeholders and to avoid an expectation gap at delivery, you need to categorize our User Stories using the MoSCoW method. Read all the user stories stored in `./Week4/artifacts/Example-Problem-C/user_stories.md`. Then compare the user stories with all the requirement interview transcripts (`./Week4/artifacts/Example-Problem-C/docs/transcripts/customer_support.md`, `Week4/artifacts/Example-Problem-C/docs/transcripts/engineering.md`, and `Week4/artifacts/Example-Problem-C/docs/transcripts/finance.md`). Then label each user story by applying the MoSCoW method:
Categorization Rules:
- M (Must Have): Non-negotiable core functionality. Without these, the product is not viable.
- S (Should Have): Important but not vital for the initial launch.
- C (Could Have): Desirable 'bonus' features that will be included only if resources permit.
- W (Won't Have): Explicitly excluded from this release because 1. the user story is not realistic, 2. there's a need to further confirm with the client, 3. the need is out-of-scope at the moment. 

Fill your label into the column `Priority` in `./Week4/artifacts/Example-Problem-C/user_stories.md`. 

**Task:**

Label each user story with a MoSCoW label, to determine the priority of each user story in order to determine requirement priority. The user story file should be generated by the upstream question in Problem 3. 

**Solution or Output generated by Github Copilot:**

**1. Core Recognition & Wiki Integration**

| User Story # | Description | Story Points | Priority | Acceptance Criteria |
|-------------|-------------|--------------|--------|-------------------|
| 1.01 | As a casual learner, I want to point my camera at an object and have it identified so that I can automatically land on its specific Wikipedia paragraph. | 3 | M | Given: The app is open with the camera active and a clear view of a common object (e.g., a spoon). When: The user centers the object in the viewfinder, and the AI identifies it with high confidence. Then: The app must automatically launch the in-app browser and scroll directly to the relevant descriptive paragraph on the localized Wikipedia page. |
| 1.02 | As a user, I want the identification process to take less than 0.5 seconds so that the experience feels magical and instantaneous. | 5 | S | Given: The user is on a standard high-speed mobile network or Wi-Fi. When: An object is captured in the camera frame. Then: The time elapsed from "Image Capture" to "Identity Displayed" must be <= 0.5 seconds. Then: The UI must maintain a smooth frame rate during this transition to ensure a "theatrical" and "magical" feel. |
| 1.03 | As a researcher, I want the app to generate a "micro-article" if a Wikipedia page doesn't exist so that I still get immediate context about the object. | 8 | S | Given: The AI identifies an object (e.g., a specific prototype) that does not have an existing Wikipedia entry. When: The retrieval engine confirms a 404 or "Page Not Found" status from the Wiki API. Then: The app must trigger a Large Language Model (LLM) to synthesize a brief, accurate introduction based on identified visual metadata. Then: The generated text must be clearly labeled as "AI-Generated Insight" and include an option for the user to "Submit to Wikipedia". |

**2. Advanced Visual Features**

| User Story # | Description | Story Points | Priority | Acceptance Criteria |
|-------------|-------------|--------------|--------|-------------------|
| 2.01 | As a tech enthusiast, I want a "see-through" visual overlay of internal components (e.g., CPU, RAM) so that I can understand the inner workings of my devices. | 13 | W | Given: The user points the camera at a supported closed device (e.g., a specific laptop or smartphone model). When: The AI identifies the specific make and model. Then: The app must render a "theatrical" AR overlay showing a 3D exploded schematic of the internal components (CPU, RAM, battery) aligned with the physical object. Then: The UI must display an animation of "electrons flowing" or heat maps to create the intended "wow factor". |
| 2.02 | As a style-conscious user, I want the app background to automatically detect and match my phone case's color/texture so the UI feels bespoke. | 13 | W | Given: The user has opted into "Bespoke Theming" during onboarding. When: The user provides a reference image of their phone case (the "case selfie"). Then: The app must analyze and sample the color, texture (e.g., fabric weave), and sheen of the case. Then: The app background must automatically morph to a synthesized matching wallpaper that includes the case's specific gradients or patterns. |

**3. Support & Error Handling**

| User Story # | Description | Story Points | Priority | Acceptance Criteria |
|-------------|-------------|--------------|--------|-------------------|
| 3.01 | As a user, I want to see the top three identification guesses with confidence scores if the AI is uncertain so that I can manually select the correct one. | 2 | M | Given: The AI's primary identification confidence score for an object is below the "Automatic Redirection" threshold (e.g., < 85%). When: The recognition process completes. Then: Instead of redirecting to Wikipedia, the app must display an "Ambiguity View" showing three cards representing the top three guesses. Then: Each card must display the object name and its corresponding confidence percentage (e.g., "Mug - 72%"). When: The user taps one of the cards. Then: The app immediately navigates to the Wikipedia page for that specific selection. |
| 3.02 | As an unsatisfied user, I want a "Report wrong result" flow so that I can help improve the AI's accuracy for future scans. | 3 | M | Given: The app has incorrectly identified an object or the user is on a Wikipedia page that doesn't match their physical object. When: The user taps the "Report wrong result" button. Then: A simple feedback overlay appears, allowing the user to suggest the correct name or select a category. When: The user submits the report. Then: The app must bundle the original image, the AI's guessed label, and the user's feedback, then upload it to the "Failure Case" database. Then: The user must receive a confirmation message (e.g., "Thanks! This helps WikiLens get smarter."). |

**4. Privacy & Accessibility**

| User Story # | Description | Story Points | Priority | Acceptance Criteria |
|-------------|-------------|--------------|--------|-------------------|
| 4.01 | As a privacy-conscious user, I want an option to opt out of server-side logging so that my scanned images are not stored on the cloud. | 3 | M | Given: The user navigates to the "Privacy" section of the app settings. When: The user toggles the "Server-side Logging" switch to the OFF position. Then: Any subsequent images sent to the cloud for recognition must be deleted immediately after the identification result is returned. Then: The system must ensure that these images are not added to the 30-day retention pool or the manual curation pipeline. |
| 4.02 | As a non-English speaker, I want the app to support my native language (top 5 markets) so that I can use the tool effectively regardless of my location. | 5 | C | Given: The app is being used in one of the top five markets (English, Spanish, Mandarin, Hindi, Arabic). When: The user selects their preferred language in the system settings or app settings. Then: All UI elements (buttons, onboarding, error messages) must be displayed in that language. Then: When an object is identified, the app must automatically retrieve and display the Wikipedia entry from that language's specific Wiki subdomain (e.g., es.wikipedia.org). |

**Why it's better**

The previous guideline only prioritizes different requirements. The new guideline provides a framework of not only prioritizing user stories and requirements, but also identifies requirements that are 1. unrealistic, 2. ambiguious, and 3. need further fine-grinding, to push back the clients. 

---

#### Guideline 3: Task-Defined Output Format

**Prompt and Context:**

Classify R1–R4 in `artifacts/requirements_R1-R4.md` as F or N. This task requires labels only — output ONLY a 2-column table (ID | F or N). Do not add justifications; the task says not to.

**Task:** Classify R1–R4 as F or N. Produce only a 2-column table: ID | F or N. No justifications, no explanation, no reflection.

**Solution or Output generated by GitHub Copilot:**

| ID | F or N |
| --- | --- |
| R1 | F |
| R2 | N |
| R3 | F |
| R4 | N |

This solution correctly classifies all requirements and provides only the labels as requested. No justifications are included, matching the task requirement for a labels-only output suitable for traceability matrices or tickboxes.

...

---
